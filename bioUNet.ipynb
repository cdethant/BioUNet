{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import json\n","import os\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from torch.nn.functional import relu\n","from torchvision import transforms"]},{"cell_type":"markdown","metadata":{},"source":["Below is a simple UNet from documentation used for training segmentation of MRI:\n","\n","Work Cited: https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from collections import OrderedDict\n","\n","class UNet(nn.Module):\n","\n","    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n","        super(UNet, self).__init__()\n","\n","        features = init_features\n","\n","        # ._block is defined below, shortening the need for repeating Conv, Norm, and Activation calls.\n","\n","        # Encoder Block\n","        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n","        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Bridge/Bottleneck\n","        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n","\n","        # Decoder Layer\n","        self.upconv4 = nn.ConvTranspose2d(\n","            features * 16, features * 8, kernel_size=2, stride=2\n","        )\n","        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n","        self.upconv3 = nn.ConvTranspose2d(\n","            features * 8, features * 4, kernel_size=2, stride=2\n","        )\n","        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n","        self.upconv2 = nn.ConvTranspose2d(\n","            features * 4, features * 2, kernel_size=2, stride=2\n","        )\n","        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n","        self.upconv1 = nn.ConvTranspose2d(\n","            features * 2, features, kernel_size=2, stride=2\n","        )\n","        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n","\n","        self.conv = nn.Conv2d(\n","            in_channels=features, out_channels=out_channels, kernel_size=1\n","        )\n","\n","    def forward(self, x):\n","        enc1 = self.encoder1(x)\n","        enc2 = self.encoder2(self.pool1(enc1))\n","        enc3 = self.encoder3(self.pool2(enc2))\n","        enc4 = self.encoder4(self.pool3(enc3))\n","\n","        bottleneck = self.bottleneck(self.pool4(enc4))\n","\n","        dec4 = self.upconv4(bottleneck)\n","        dec4 = torch.cat((dec4, enc4), dim=1)\n","        dec4 = self.decoder4(dec4)\n","        dec3 = self.upconv3(dec4)\n","        dec3 = torch.cat((dec3, enc3), dim=1)\n","        dec3 = self.decoder3(dec3)\n","        dec2 = self.upconv2(dec3)\n","        dec2 = torch.cat((dec2, enc2), dim=1)\n","        dec2 = self.decoder2(dec2)\n","        dec1 = self.upconv1(dec2)\n","        dec1 = torch.cat((dec1, enc1), dim=1)\n","        dec1 = self.decoder1(dec1)\n","        return torch.sigmoid(self.conv(dec1))\n","\n","    @staticmethod\n","    def _block(in_channels, features, name):\n","        return nn.Sequential(\n","            OrderedDict(\n","                [\n","                    (\n","                        name + \"conv1\",\n","                        nn.Conv2d(\n","                            in_channels=in_channels,\n","                            out_channels=features,\n","                            kernel_size=3,\n","                            padding=1,\n","                            bias=False,\n","                        ),\n","                    ),\n","                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n","                    (name + \"relu1\", nn.ReLU(inplace=True)),\n","                    (\n","                        name + \"conv2\",\n","                        nn.Conv2d(\n","                            in_channels=features,\n","                            out_channels=features,\n","                            kernel_size=3,\n","                            padding=1,\n","                            bias=False,\n","                        ),\n","                    ),\n","                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n","                    (name + \"relu2\", nn.ReLU(inplace=True)),\n","                ]\n","            )\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["The above code is long and tedious for defining a UNet layer by layer; thus can be simplified into funtions below:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def conv_block(input, num_filters):\n","    s = nn.Conv2d(num_filters, kernel_size=3, padding = 1)(input)\n","    s = nn.BatchNorm2d(num_filters)\n","    s = nn.ReLU()(s)\n","\n","    s = nn.Conv2d(num_filters, kernel_size=3, padding = 1)(s)\n","    s = nn.BatchNorm2d(num_filters)\n","    s = nn.ReLU()(s)\n","\n","    return s\n","\n","def encoder_block(input, num_filters):\n","    s = conv_block(input, num_filters)\n","    p = nn.MaxPool2d(kernel_size=2, stride=2)(s)\n","\n","    return s, p\n","\n","def decoder_block(input, num_filters, skip_connections):\n","    d = nn.ConvTranspose2d(num_filters, kernel_size=2, stride=2)(input)\n","    d = torch.cat([d, skip_connections])\n","    d = conv_block(input, num_filters)\n","\n","    return d\n","\n","\n","def create_UNet(input_shape, n_classes):\n","    inputs = Input(input_shape)\n","\n","    s1, p1 = encoder_block(inputs, 64)\n","    s2, p2 = encoder_block(p1, 128)\n","    s3, p3 = encoder_block(p2, 256)\n","    s4, p4 = encoder_block(p3, 512)\n","\n","    b1 = conv_block(p4, 1024)\n","\n","    d1 = decoder_block(b1, 1024, s4)\n","    d2 = decoder_block(d1, 512, s3)\n","    d3 = decoder_block(d2, 256, s2)\n","    d4 = decoder_block(d3, 128, s1)\n","\n","    output = nn.Conv2d(64, n_classes, kernel_size=1)\n","\n","    model = \n","\n","    return model"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"'mask_path'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_structure\u001b[39m(d, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Print the structure of a dictionary or list.\"\"\"\u001b[39;00m\n","\u001b[1;31mKeyError\u001b[0m: 'mask_path'"]}],"source":["import json\n","\n","with open('C:\\\\Users\\\\ethxn\\\\Desktop\\\\Programming projects\\\\BioUNet\\\\archive\\\\Nuclei\\\\Nuclei\\\\annotations\\\\stage_1_test.json', 'r') as file:\n","    data = json.load(file)\n","\n","for img in data['images'][:10]:\n","    print(img['file_name'])\n","\n","def print_structure(d, indent=0):\n","    \"\"\"Print the structure of a dictionary or list.\"\"\"\n","    \n","    # If the input is a dictionary\n","    if isinstance(d, dict):\n","        for key, value in d.items():\n","            print('  ' * indent + str(key))\n","            print_structure(value, indent+1)\n","            \n","    # If the input is a list\n","    elif isinstance(d, list):\n","        print('  ' * indent + \"[List of length {} containing:]\".format(len(d)))\n","        if d:\n","            print_structure(d[0], indent+1)  # Only print the structure of the first item for brevity\n","\n","print_structure(data)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pandas'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"]}],"source":["class NucleiSegmentData:\n","    def __init__(self, transforms=None):\n","    \n","    def __len__():\n","\n","\n","    def __getitem__():\n","\n","\n","transformations = torchvision.transforms.Compose([\n","    ToTensor()\n","])\n","\n","#importing the datasets\n","train_dataset = torchvision.datasets.NucleiSegmentData( transform = transformations)\n","val_dataset = torchvision.datasets.NucleiSegmentData( transform = transformations)\n","\n","#we need to use a dataloader to load in the datasets - associates labels with dataset images\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 64)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 64)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train():\n","    model = create_UNet()\n","    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","    cec = nn.CrossEntropyLoss()\n","    accuracies = []\n","    max_accuracy = 0\n","\n","    device = next(model.parameters()).device\n","\n","    num_epochs = 10\n","\n","    for epoch in range(num_epochs):\n","        # Train Mode\n","        model.train()\n","\n","        for i, (images, labels) in enumerate(train_dataloader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            #No gradient descent\n","            optimizer.zero_grad()\n","\n","            #Calculate Loss\n","            pred = model(images)\n","            loss = cec(pred, labels)\n","\n","            #Backpropagation\n","            loss.backward()\n","\n","            #Adjust & optimize model parameters\n","            optimizer.step()\n","\n","\n","        model.eval()\n","        running_loss = 0.0\n","\n","        with torch.no_grad():\n","            for inputs, targets in val_dataloader:\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","                running_loss += loss.item() * inputs.size(0)\n","                accuracy = \n","\n","        avg_loss = running_loss / len(dataloader.dataset)\n","        print(f'Validation Loss: {avg_loss:.4f}')\n","    \n","        accuracies.append(accuracy)\n","        \n","        # Find best model\n","        if (accuracy > max_accuracy):\n","            max_accuracy = accuracy\n","            best_model = copy.deepcopy(model)\n","            print(f'Epoch: {epoch + 1}, Accuracy: {accuracy}')\n","\n","\n","\n","\n","#Validation\n","def val_loop(model, data):\n","    model.eval() #switch from training to validation mode\n","    total = 0\n","    correct = 0\n","\n","    #Declaring the device\n","    device = next(model.parameters()).device\n","\n","    with torch.no_grad(): #No gradient descent needed\n","        for images, labels in data:\n","            #Puts each image on CUDA\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","\n","    return 100 * correct / total"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":2}
